{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb395dc6-640c-42a0-83a2-1f66b3f6128d",
   "metadata": {},
   "source": [
    "# get_CottonQualityData_from_NASS\n",
    "\n",
    "**Project:** Texas and Georgia Agriculture\n",
    "\n",
    "**Date:** March 04, 2025\n",
    "\n",
    "**Code Contact:** Kelechi Igwe, [igwekelechi.e@gmail.com]\n",
    "\n",
    "**Inputs:** \n",
    "> Folder path to where all output CSV files should be stored.\n",
    "        \n",
    "**Outputs:** A CSV file containing weekly cotton quality data for Texas and Georgia, over the number of years available on the website.\n",
    "\n",
    "**Description:** \n",
    "\n",
    "This script will extract Cotton quality data for all available years and by districts for Texas and Georgia states from the National Agricultural Statistics Service (NASS) website here: https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4a0056-6d51-4bf7-af24-5b08adf0b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the libraries below, if not already installed\n",
    "\n",
    "#!pip install wget\n",
    "#pip install wget, requests, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc01bee-6b14-4379-94fb-936c8edacc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "#import wget # this does the actual download\n",
    "import re #This collects all links within a text\n",
    "import requests # access the website\n",
    "from urllib.parse import urljoin, unquote #to join url links together\n",
    "import chardet # access the encoding of a text file\n",
    "import json # temperarily save file in this format before using it.\n",
    "import io # to read web object as file, temporarily\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbccf40c-f4d0-4d00-9108-33dc79612203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the link to the website below\n",
    "\n",
    "url = 'https://apps.ams.usda.gov/'\n",
    "base_url = 'https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aecc95f-78dd-4978-869a-7d59dfc7cdaf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 1: Save URL links of weekly files for each year into a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e29ade0-3ed6-4c4e-b07b-23cc69ead4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to collect all website information from the provided link\n",
    "\n",
    "r = requests.get(base_url)\n",
    "if r.status_code != 200: # status code 200 means access is sucessful\n",
    "    print(\"Website url is bad\")\n",
    "    exit()\n",
    "\n",
    "# Collect links representing all the years available into a list\n",
    "links = re.findall(r'href=[\"\\'](.*?)[\"\\']', r.text, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74d3e2c8-478d-4219-8fc4-b76eabc4bfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Cotton/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2015%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2016%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2017%20crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2018%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2019%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2020%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2021%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2022%20Crop/',\n",
       " '/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2023%20Crop/']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02048433-74e9-4a95-b730-88a9eb6e1bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2015%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2016%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2017%20crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2018%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2019%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2020%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2021%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2022%20Crop\n",
      "https://apps.ams.usda.gov/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/2023%20Crop\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "THIS PART OF THE SCRIPT WILL EXTRACT THE LINKS TO ALL THE WEEKS AVAILABALE IN A YEAR WHICH WILL BE USED TO MAKE A DOWNLOAD REQUEST \n",
    "'''\n",
    "# Input any years you already have data for, otherwise leave the list empty\n",
    "years_available = [2014, 2013, 2025]\n",
    "\n",
    "# store all weeks for each year here \n",
    "dictionary_of_years = {}\n",
    "\n",
    "# Loop through all the listed links extract the links to the specific year folders\n",
    "for link in links:\n",
    "    \n",
    "    files_in_year = None # Initialize the value\n",
    "\n",
    "    # Only select links that are actual cotton folders\n",
    "    if link != \"/Cotton/\":\n",
    "        \n",
    "        # Get the current year\n",
    "        year = int(link.split('/')[-2].split('%20')[0])\n",
    "\n",
    "        # Check if current year is already in the list of downloaded years (might take out this part later)\n",
    "        if year not in years_available:\n",
    "            # Create a link for the current year which has not been downloaded\n",
    "            files_in_year = urljoin(base_url, link.split('/')[-2])\n",
    "            \n",
    "            # Check if the created link is valid before continuing on\n",
    "            if files_in_year: # Code below will only run if there are links in files_in_year\n",
    "                \n",
    "                # Get the folder name (this will be used to create a folder to store the weekly text files)\n",
    "                folder_name = unquote(os.path.basename(files_in_year))\n",
    "                \n",
    "                #print('Saving: ', folder_name) # This is to keep track of which year is being processed.\n",
    "            \n",
    "                # Make a web request to get all the week files in the current year\n",
    "                r2 = requests.get(files_in_year)\n",
    "                if r2.status_code != 200: # status code 200 means access is sucessful\n",
    "                    print(\"Website url is bad\")\n",
    "                    exit()\n",
    "            \n",
    "                # Collect all the links representing all weeks for the current year into a list\n",
    "                weeks_links = re.findall(r'href=[\"\\'](.*?)[\"\\']', r2.text, re.IGNORECASE)\n",
    "            \n",
    "                # Loop through the list of weekly data and collect the links to each week in a list\n",
    "                list_of_weeks = []\n",
    "                for week in weeks_links:\n",
    "                    if week != \"/Cotton/Weekly_Cotton_Quality_Data_files_by_NASS_Ag_Districts/\":\n",
    "                        current_week = urljoin(url, week)\n",
    "                        filename = unquote(os.path.basename(current_week))\n",
    "            \n",
    "                        # Attach the current week to the list of weeks\n",
    "                        list_of_weeks.append(current_week)\n",
    "            \n",
    "                # Append the list of weeks in the current year to its corresponding year in the dictionary\n",
    "                dictionary_of_years[folder_name] = list_of_weeks\n",
    "            \n",
    "            # print links for year in dictionary_of_years\n",
    "            print(files_in_year)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c149f818-0633-4b01-a69c-c8c1e270dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After confirming the the links for all weeks in a year are in the dictionary, go to section 3\n",
    "#dictionary_of_years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047cfdf-17e1-4159-ab25-251d85c4497a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 2: this section will download all the raw files for the links in the dictionary above (Please see note!!!!)\n",
    "**NOTE:** if you want to extract for only Texas and Georgia before downloading files, skip this section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2776e-5010-4fdb-b311-1c9415dfcd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year,files_list in dictionary_of_years.items():\n",
    "    print('processing: ', year)\n",
    "    \n",
    "    # Create a folder for the current year if it doesn't exist already\n",
    "    os.makedirs(year, exist_ok=True)\n",
    "\n",
    "    for file in files_list:\n",
    "        # Get file title for the current week\n",
    "        filename = unquote(os.path.basename(file))\n",
    "    \n",
    "        # Uniquely name the textfiles before saving to folder\n",
    "        text_file_path = os.path.join(year, filename)\n",
    "\n",
    "        \n",
    "        # Get the content of the single week's file\n",
    "        r3 = requests.get(file)\n",
    "        \n",
    "        \n",
    "        # Write the web results to text file\n",
    "        if r3.status_code == 200:\n",
    "            with open(text_file_path, \"wb\") as entry:\n",
    "                entry.write(r3.content)\n",
    "                \n",
    "            print(f'{text_file_path} completed!')\n",
    "            \n",
    "        else:\n",
    "            print('Falied to access link')\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7a493-f05a-4a13-bf26-94d94466f8c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section 3: Run this part to extract data for only Texas and Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af0d7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  2015 Crop\n",
      "processing:  2016 Crop\n",
      "processing:  2017 crop\n",
      "processing:  2018 Crop\n",
      "processing:  2019 Crop\n",
      "processing:  2020 Crop\n",
      "processing:  2021 Crop\n",
      "processing:  2022 Crop\n",
      "processing:  2023 Crop\n",
      "Script run is successful! Here's how the 2023 Crop data looks: \n",
      " Empty DataFrame\n",
      "Columns: [Week Beginning, Week Ending, State-NASS District Number, Official Color Grade, Leaf Grade, Extraneous Matter, Remarks, MIKE, Strength, HVI Color Code, HVI Color Quad, HVI Color RD, HVI Color +b, Trash % Surface, Length 100ths, Length Uniformity, Filtered out at 11:12:01 AM]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "THIS PART OF THE SCRIPT WILL START DOWNLOADING FILES FOR EACH WEEK IN THE YEARS AVAILABLE IN 'dictionary_of_years'\n",
    "'''\n",
    "\n",
    "for year,files_list in dictionary_of_years.items():\n",
    "    print('processing: ', year)\n",
    "    \n",
    "    # Create a folder for the current year if it doesn't exist already\n",
    "    os.makedirs(year, exist_ok=True)\n",
    "\n",
    "    for file in files_list:\n",
    "        # Get file title for the current week\n",
    "        filename = unquote(os.path.basename(file)).split('.')[0]\n",
    "    \n",
    "        # Uniquely name the textfiles before saving to folder\n",
    "        text_file_path = os.path.join(year, filename)\n",
    "\n",
    "        # Get the content of the single week's file\n",
    "        r3 = requests.get(file)\n",
    "        \n",
    "        \n",
    "        # temporarily write the requested data to pickle format\n",
    "        with open('temp.pickle', 'wb') as f:\n",
    "            pickle.dump(r3.content, f)\n",
    "        \n",
    "        # Read the pickle data\n",
    "        with open('temp.pickle', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "            # get the encoding of the file (useful to read as csv)\n",
    "            preview = data[:100000] # preview first 100 KB of data and get the file encoding format\n",
    "            file_encoding = chardet.detect(preview)['encoding']\n",
    "        \n",
    "            # The variable 'data' is of type 'bytes', but we need it as a file object to read it as csv\n",
    "            file_object = io.BytesIO(data)\n",
    "        \n",
    "        # Now we can read it as csv\n",
    "        df = pd.read_csv(file_object, delimiter = '\\t', encoding = file_encoding, on_bad_lines = 'skip')\n",
    "        #print(df.head())\n",
    "        \n",
    "        # To see all the the states contained in the file\n",
    "        unique_dist = df['State-NASS District Number'].unique()\n",
    "        \n",
    "        # Get only data for Texas and Georgia (that is, if the column for 'State-NASS District Number' contains TX)\n",
    "        df_new = df[df['State-NASS District Number'].str.contains('GA-70') | df['State-NASS District Number'].str.contains('GA-80') | df['State-NASS District Number'].str.contains('GA-90')]\n",
    "        \n",
    "        # Save the new data as text in the newly created folder\n",
    "        df_new.to_csv(f'{text_file_path}.csv', sep=',', index=False)\n",
    "    \n",
    "print(f\"Script run is successful! Here's how the {year} data looks: \\n\", df_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d30feeeb-8844-4669-9b52-6b08204029fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kelechi\\\\OneDrive - Kansas State University\\\\Desktop\\\\Research Resources\\\\Conferences\\\\NASA_DEVELOP\\\\Scripts'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see where the files are saved\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a6cb5-b388-469e-b084-2b7252e5155c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
