{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151bffff-5901-4c2e-a047-fc128a21f3cd",
   "metadata": {},
   "source": [
    "# Harmonized Landsat Sentinel (HLS) EVI extraction\n",
    "\n",
    "**Project:** Georgia and Texas Agriculture\n",
    "\n",
    "**Date:** 03/10/2025\n",
    "\n",
    "**Code Contact:** Henry Osei, henryoseipoku77@gmail.com\n",
    "\n",
    "**Inputs:** Boundary shapefile for each NASS district of interest. Cotton mask for each district that was generated in the TXGA_CottonMask.ipynb file. \n",
    "\n",
    "**Outputs:** CSV files of monthly Enhanced Vegetation Index (EVI).\n",
    "\n",
    "**Description:** This script derives EVI values per month (March to November) for the NASS districts of interest from 2015 to 2024. Firstly, the red, blue, and NIR bands are extracted from the HLS data to calculate EVI. EVI tiles for each district are mosaicked into a single EVI tile per district. Half monthly composites of EVI are then created for a finer EVI time series. Finally, all half monthly EVI csv files per year are assembled together to produce montly EVI per district.   \n",
    "- NASS districts of interest: **Georgia**: GA-70, GA-80, GA-90; **Texas**: TX-12, TX-21, TX-22, TX-60, TX-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d719f4-c055-4868-8cf5-b0a0bf15bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import dask.array as da\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.transform import from_origin\n",
    "import rioxarray as rxr\n",
    "import earthaccess\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import dask.delayed\n",
    "from rasterio.plot import show\n",
    "from rasterio.crs import CRS\n",
    "import xarray as xr\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c40881-5616-4663-afb6-499374cfb2bb",
   "metadata": {},
   "source": [
    "### PART A\n",
    "- This is where we will calculate EVI and create csv files of half monthly EVI composites. \n",
    " \n",
    "**NB:** The processing in this section is computationally demanding, so unless you are working on a 'supercomputer' (RAM >16GB, speed >3.0, CPUs> 12, etc.), it is recommended that you do this section per district as was done here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4e20c-9516-443d-a19c-ec0dcd177800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base directory for all the input files\n",
    "# NB: On your computer, change this to the directory of the input files \n",
    "path= os.chdir('C:/TX_GA_CSB/per_district')\n",
    "\n",
    "# create a folder to store the output files\n",
    "int_dir = \"Image_files\" # one to store the intermediate output files\n",
    "os.makedirs(int_dir)\n",
    "\n",
    "final_dir = \"EVI_CSV\" # another to store the final output files\n",
    "os.makedirs(final_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66fd4b-3868-423f-89d3-8e3f4c523556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the GA_90 district boundary and cotton mask shapefiles\n",
    "GA_90 = gpd.read_file('GA_90.shp')\n",
    "GA_90_farms = gpd.read_parquet('GA_90_CottonMask.geoparquet')\n",
    "\n",
    "\n",
    "# we will need the bounding box to extract all HLS data that intersects with the district, so\n",
    "# extract the bounding box for GA_90 district\n",
    "GA_90bbox = tuple(GA_90.total_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f76545-ab02-48b1-84cd-b3475b628526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB: before you can use the earthaccess python API to extract HLS data, you need to have an Earthdata account\n",
    "# authenticate earthaccess\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d2993-6b8c-4dc3-9cc6-ec5d1baf20f0",
   "metadata": {},
   "source": [
    "**NB:** The HLS dataset can provide data every to 2 to 3 days (starting from 2018). Also, one district can have up to 8 different tiles per acquisition date. Lastly, we need three different bands per tile to calculate EVI, meaning that we will be accessing a huge amount of raster files to calculate EVI at the district level.\n",
    "\n",
    "So, if you are not using a supercomputer, it is recommended that you break the HLS data retrieval and processing into quarterly increments per year, as was done in this code. You can however retrieve data for all the desired months in one search for the year 2015, as Sentinel-2 data was not available until November 2015.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c69a2-c3d5-467f-b6b3-f054b3057755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# use earthaccess to search and open all available HLS data that intersects with the GA_90 district in 2015, in quarterly timesteps\n",
    "# --------------------------------\n",
    "\n",
    "# data from Landsat 8/9, HLSL30\n",
    "L30_results = earthaccess.search_data(\n",
    "    short_name='HLSL30',\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2015-03-01\", \"2015-05-31\"), # March to May\n",
    "    bounding_box=GA_90bbox,\n",
    ")\n",
    "\n",
    "# data from Sentinel-2, HLSS30\n",
    "S30_results = earthaccess.search_data(\n",
    "    short_name='HLSS30',\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2015-03-01\", \"2015-05-31\"),\n",
    "    bounding_box=GA_90bbox,\n",
    ")\n",
    "\n",
    "# Combine S30 and L30 products into a time series\n",
    "HLS_items = L30_results + S30_results\n",
    "# len(HLS_items) #check the number of tiles in the specified timeframe\n",
    "\n",
    "# define EVI function\n",
    "def calc_EVI(red, blue, nir):\n",
    "      return 2.5 * (nir - red) / (nir + 6.0 * red - 7.5 * blue + 1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d844a7-552c-48ad-82d1-c5673d1f13f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5a46ce0-2ed1-4504-b0ae-5f5af3b7730c",
   "metadata": {},
   "source": [
    "**NB:** Some of the districts are located in more than one UTM coordinate system, so for a district, you can have some image tiles in UTM zone 13 and others in UTM zone 14. To accurately mosaic individual tiles of a district, we need to reproject all the image tiles into one coordinate system (WGS84). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b280f-ff5c-4ca2-8aad-b11fcbe47de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# define a function to extract the nir, red, and blue bands of the HLS data.\n",
    "# And for each acquisition date image composite (nir, red, blue bands), \n",
    "# calculate EVI and reproject the current coordinate reference system (crs) of the EVI to WGS84.\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# crs to use later for reprojection of EVI images\n",
    "wgs84_crs= CRS.from_epsg(4326)\n",
    "\n",
    "# Define a function to process the nir, red, and blue bands for each acquisiton date granule into an EVI band and save on disk.\n",
    "def process_granule_to_disk(gran):\n",
    "    # Determine product type (HLSL30 or HLSS30) and set desired spectral bands accordingly\n",
    "    if gran.get('umm')['RelatedUrls'][0]['URL'].split('/')[4] == 'HLSS30.020':\n",
    "        desired_bands = ['B8A', 'B04', 'B02']  # Bands for HLSS30 product\n",
    "    else:\n",
    "        desired_bands = ['B05', 'B04', 'B02']  # Bands for HLSL30 product\n",
    "\n",
    "    # extract urls for the desired bands, excluding 's3://' urls (we don't want the urls on amazon S3)\n",
    "    band_urls = [\n",
    "        url['URL']\n",
    "        for url in gran.get('umm')['RelatedUrls']\n",
    "        if any(band in url['URL'] for band in desired_bands) and 's3://' not in url['URL']\n",
    "    ]\n",
    " \n",
    "    # use earthaccess to create vsicurls so that we can stream the data (desired_bands) without downloading it onto our computer\n",
    "    vsicurls = earthaccess.open(band_urls)\n",
    "\n",
    "    # initialize arrays to store the band data and crs information\n",
    "    # NB: we will need the crs of the current granule before we can reproject the crs of EVI \n",
    "    nir_array = red_array = blue_array = None\n",
    "    original_crs = None\n",
    "\n",
    "    # open and read the bands vsicurl into memory, apply scale factor and replace nodata values with nan\n",
    "    for url in vsicurls:\n",
    "        band_type = str(url).rsplit('.', 2)[-2] # extract the band number\n",
    "        with rasterio.open(url) as src:\n",
    "            band_data = src.read(1)\n",
    "            band_data = np.where(band_data == src.nodata, np.nan, band_data)  # replace nodata values with nan\n",
    "            band_scaled = da.from_array(band_data * src.scales[0], chunks=(500, 500))  # scale data. Use dask to break the data into chunks for fast processing\n",
    "\n",
    "            # assign data to respective arrays based on band type\n",
    "            if band_type in ('B8A', 'B05'):\n",
    "                nir_array = band_data\n",
    "                nir_scaled = band_scaled\n",
    "            elif band_type == 'B04':\n",
    "                red_array = band_data\n",
    "                red_scaled = band_scaled\n",
    "            elif band_type == 'B02':\n",
    "                blue_array = band_data\n",
    "                blue_scaled = band_scaled\n",
    "\n",
    "            # extract the crs information from the nir band\n",
    "            if original_crs is None:\n",
    "                original_crs = src.crs\n",
    "\n",
    "    # compute the EVI using scaled band data\n",
    "    nir_scaled, red_scaled, blue_scaled = dask.compute(nir_scaled, red_scaled, blue_scaled)\n",
    "    evi_band = calc_EVI(red_scaled, blue_scaled, nir_scaled)\n",
    "\n",
    "    # reproject the EVI data to WGS84\n",
    "    transform, width, height = rasterio.warp.calculate_default_transform(\n",
    "        original_crs, wgs84_crs, evi_band.shape[1], evi_band.shape[0], *src.bounds\n",
    "    )\n",
    "    evi_reprojected = np.empty_like(evi_band)\n",
    "    reproject(\n",
    "        source=evi_band,\n",
    "        destination=evi_reprojected,\n",
    "        src_transform=src.transform,\n",
    "        src_crs=original_crs,\n",
    "        dst_transform=transform,\n",
    "        dst_crs=wgs84_crs,\n",
    "        resampling=Resampling.nearest\n",
    "    )\n",
    "\n",
    "    # Save the processed EVI data to disk as a GeoTIFF file\n",
    "    gran_name = str(url).rsplit('/', 1)[-1][:-1]  # extract the product name from the band url\n",
    "    out_name = os.path.join(int_dir,f\"{gran_name.split('.B')[0]}_EVI.tif\") # select the necessary name parts, add 'EVI' to the name, and save to the 'Image_files directory\n",
    "\n",
    "    with rasterio.open(out_name, 'w', driver='GTiff', height=evi_reprojected.shape[0], \n",
    "                       width=evi_reprojected.shape[1], count=1, dtype=evi_reprojected.dtype, \n",
    "                       crs=wgs84_crs, transform=transform) as dst:\n",
    "        dst.write(evi_reprojected, 1)\n",
    "\n",
    "    return out_name\n",
    "\n",
    "# process all granules in parallel using Dask\n",
    "results = dask.compute(*[dask.delayed(process_granule_to_disk)(gran) for gran in HLS_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79323e4d-79b5-4f08-8602-ef00bf7f3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# groups the file names based on acquisition date, mosaic them and clip them to the bounding box of the study area\n",
    "# --------------------------------\n",
    "\n",
    "# change the base directory to the Image_files folder\n",
    "path= os.chdir('C:/TX_GA_CSB/per_district/Image_files')\n",
    "\n",
    "# read all file names with '_EVI.tif' into a list\n",
    "evi_files = glob.glob('*_EVI.tif')\n",
    "\n",
    "# group the EVI filenames by Julian day (day of year, DOY)\n",
    "date_groups = {}\n",
    "for file in evi_files:\n",
    "    j_date = int(file.split('.')[3][4:7])  # extract DOY from file name\n",
    "    if j_date not in date_groups:\n",
    "        date_groups[j_date] = []  # initialize group if DOY not seen before\n",
    "    date_groups[j_date].append(file)\n",
    "\n",
    "# mosaic grouped EVI files for each DOY and save as GeoTIFF\n",
    "for date, filepaths in date_groups.items():\n",
    "    # open the raster files for the current group\n",
    "    sources = [rasterio.open(path) for path in filepaths]\n",
    "\n",
    "    # create a mosaic\n",
    "    mosaic, out_trans = merge(sources)\n",
    "\n",
    "    # close the raster files \n",
    "    for src in sources:\n",
    "        src.close()\n",
    "\n",
    "    # define file name for the output mosaic using the acquisition date (DOY)\n",
    "    mosaic_file = f\"mosaic_{date}.tif\"\n",
    "\n",
    "    # update metadata for the mosaic file\n",
    "    mosaic_meta = sources[0].meta.copy()\n",
    "    mosaic_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans,\n",
    "        \"dtype\": mosaic.dtype\n",
    "    })\n",
    "\n",
    "    # save the mosaic as a  GeoTIFF file\n",
    "    with rasterio.open(mosaic_file, \"w\", **mosaic_meta) as dst:\n",
    "        dst.write(mosaic)\n",
    "\n",
    "\n",
    "# read all file names with 'mosaic.tif' into a list\n",
    "mosaic_files = glob.glob('mosaic*.tif')\n",
    "\n",
    "# clip the mosaics to the bounding box of GA_90 and store the results\n",
    "clipped_evis = {}  # dictionary to store the clipped EVIs\n",
    "for date, file in zip(list(date_groups.keys()), mosaic_files):\n",
    "        mosaic_rxr = rxr.open_rasterio(file, masked=True) # open the mosaic raster with rioxarray\n",
    "\n",
    "        # clip the mosaic using the bbox of GA_90\n",
    "        mosaic_clipped = mosaic_rxr.rio.clip_box(*GA_90.total_bounds)\n",
    "\n",
    "        # some of the mosaicked EVIs does not cover the entire bbox of GA_90, so    \n",
    "        # replace no-data values with NaN\n",
    "        mosaic_clipped = mosaic_clipped.where(mosaic_clipped != 0, float(\"nan\"))\n",
    "\n",
    "        # EVI values can sometimes go beyond 1 when there are very bright objects, so \n",
    "        # mask values outside the valid range (-1 to 1)\n",
    "        mosaic_clipped = mosaic_clipped.where((mosaic_clipped >= -1) & (mosaic_clipped <= 1), float(\"nan\"))\n",
    "\n",
    "        # store the clipped mosaic with the key as the acquistion date (DOY)\n",
    "        clipped_evis[date] = mosaic_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a767bc-b087-47d8-a6be-f7bc680c78a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e6c1cc-9d7d-4fee-b1f0-952a3b624ff9",
   "metadata": {},
   "source": [
    "**NB**: We will now overlay the clipped EVI rasters for each half-month period, selecting the maximum pixel values to generate half-monthly EVI rasters. This process helps minimize the influence of cloud cover. Since the shapes (numbers of rows and columns) of the clipped EVI rasters differ, we need to standardize them to the same raster size before performing the overlay  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0442e-f193-4b1e-a234-673e30966aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the bbox of GA_90 in meters (UTM) to calculate the raster size of the target raster grid for storing the clipped EVI files, so\n",
    "# reproject the GA_90 from WGS84 to UTM \n",
    "# NB: we need the projected GA_90 just to calculate the target raster size, after that, we go back to working with the GA_90 in WGS84\n",
    "crs_schema= CRS.from_epsg(32617)\n",
    "GA_90_proj = GA_90.to_crs(crs_schema)\n",
    "GA_90_proj_bbox= tuple(GA_90_proj.total_bounds)\n",
    "\n",
    "# calculate the size of the raster\n",
    "width = int((GA_90_proj_bbox[2] - GA_90_proj_bbox[0]) / 30), # no of rows\n",
    "height = int((GA_90_proj_bbox[3] - GA_90_proj_bbox[1]) / 30) # no of columns\n",
    "\n",
    "\n",
    "# define the target grid based on GA_90 bounding box\n",
    "x_coords = np.linspace(GA_90.total_bounds[0], GA_90.total_bounds[2], width) \n",
    "y_coords = np.linspace(GA_90.total_bounds[1], GA_90.total_bounds[3], height)\n",
    "\n",
    "# align and pad each clipped EVI to the target grid\n",
    "aligned_evis = {}\n",
    "for date, mosaic_data in clipped_evis.items():\n",
    "    # align the clipped data to the target grid\n",
    "    aligned_evi = mosaic_data.reindex(\n",
    "        x=x_coords, \n",
    "        y=y_coords, \n",
    "        method=\"nearest\",\n",
    "        fill_value=float(\"nan\")  # fill empty portions with NaN\n",
    "    )\n",
    "    # store the aligned EVI with the key as the acquistion date (DOY)\n",
    "    aligned_evis[date] = aligned_evi\n",
    "\n",
    "# aligned_evis.keys()\n",
    "\n",
    "del(clipped_evis) # use this to free your memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22418571-d1a9-4d9a-9f52-2b92df735ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b445bd98-abaf-4b7a-b841-49e294fdb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# group the EVI data in intervals of 16 days (half-month) and select the maximum pixels to create half monthly EVIs\n",
    "# --------------------------------\n",
    "\n",
    "# sort the aligned EVIs using their dates\n",
    "julian_dates = sorted(aligned_evis.keys())\n",
    "\n",
    "# Define the start DOY for each month \n",
    "start_date = 60  # March 1st in Julian date\n",
    "# start_date = 152  # June 1st in Julian date\n",
    "# start_date = 244  # September 1st in Julian date\n",
    "\n",
    "\n",
    "interval = 16 # 16-day interval\n",
    "grouped_evi = {}  # dictionary to store grouped EVI data arrays\n",
    "interval_dates = []  # list to track interval start dates, we will need this later for the csv files\n",
    "current_group = []  # temporary list for EVI data arrays in the current interval\n",
    "\n",
    "# group the aligned EVIs based on intervals\n",
    "for date in julian_dates:\n",
    "    if date < start_date:\n",
    "        continue  # skip dates before the specified start date\n",
    "    if date < start_date + interval:\n",
    "        current_group.append(aligned_evis[date])  # collect EVI data that fall within the interval\n",
    "    else:\n",
    "        grouped_evi[start_date] = xr.concat(current_group, dim=\"time\")  # overlay EVIs within the same interval\n",
    "        interval_dates.append(start_date)  # track the interval start date\n",
    "        current_group = [aligned_evis[date]]  # start a new group\n",
    "        start_date += interval\n",
    "\n",
    "# combine any remaining EVI data into the last group\n",
    "if current_group:\n",
    "    grouped_evi[start_date] = xr.concat(current_group, dim=\"time\")\n",
    "    interval_dates.append(start_date)\n",
    "\n",
    "# calculate maximum EVI for each group\n",
    "max_evi_rasters = {\n",
    "    start_date: group.max(dim=\"time\", skipna=True)\n",
    "    for start_date, group in grouped_evi.items()\n",
    "}\n",
    "\n",
    "# use the cotton farms polygon data to mask/clip the maximum EVI rasters\n",
    "masked_max_evi_rasters = {}\n",
    "for start_date, max_evi_da in max_evi_rasters.items():\n",
    "    geometries = [feature for feature in GA_90_farms['geometry']]  # extract geometries to clip\n",
    "    masked_max_evi = max_evi_da.rio.clip(geometries, GA_90_farms.crs, drop=True)  # clip data\n",
    "\n",
    "    # store the masked EVI with the key as the acquistion date (DOY)\n",
    "    masked_max_evi_rasters[start_date] = masked_max_evi\n",
    "\n",
    "# average the EVI pixels in the clipped raster to get the 16-day composite evi per district\n",
    "dist_evi = {\n",
    "    start_date: masked_max_evi.mean().item()\n",
    "    for start_date, masked_max_evi in masked_max_evi_rasters.items()\n",
    "}\n",
    "\n",
    "# create a DataFrame for the 16-day composite EVI values\n",
    "GA_90_2015 = pd.DataFrame.from_dict(county_evi_means, orient='index', columns=['EVI'])\n",
    "\n",
    "# export the DataFrame as a CSV file\n",
    "output_path = os.path.join(final_dir, 'GA_90_2015_EVI_03_to_05.csv') # use the months that was specified at the start in the filename\n",
    "GA_90_2015.to_csv(output_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2a21a-f385-410f-995d-d856b33a46c7",
   "metadata": {},
   "source": [
    "After running the code till this point, delete all the files in your 'Image_files' folder to free up storage space for the images of the rest of the months and years in the district.\n",
    "Then, go back to the start to change the start and end date for searching the HLS data and run the code again till you get all the 16-day composite EVI for each year (2015 to 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a916d-3509-44c0-9b9b-a25887e90462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4ac87c1-6cdb-4efd-b91a-faeac6f717c8",
   "metadata": {},
   "source": [
    "### PART B\n",
    "- This is where we will merge all the csv files and select the maximum EVI per month to get the monthly EVI from March to November of 2015 to 2024.   \n",
    " \n",
    "**NB:** Unlike part A, this very straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd094c-4843-4cd6-b478-d2a5fa3bd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the base directory to the EVI_CSV folder\n",
    "path= os.chdir('C:/TX_GA_CSB/per_district/EVI_CSV')\n",
    "\n",
    "# load all the csv files\n",
    "GA90_files= glob.glob('*csv')\n",
    "\n",
    "# initialize an empty DataFrame to store data for all years\n",
    "GA90_years = pd.DataFrame()\n",
    "\n",
    "for year in range(2015, 2025):\n",
    "    merged_df = pd.DataFrame() # temporary DataFrame for the current year\n",
    "\n",
    "    # filter data for each year and sort the files in ascending order\n",
    "    GA90_files_sorted = sorted([file for file in GA90_files if f\"{year}\" in file])\n",
    "\n",
    "    # read each csv file and append as a DataFrame and append to the merged DataFrame\n",
    "    for file in GA90_files_sorted:\n",
    "        merged_df = pd.concat([merged_df, pd.read_csv(file)], ignore_index=True)\n",
    "            \n",
    "    merged_df.rename(columns={merged_df.columns[0]: 'Sub_month'}, inplace=True) # rename the first column (which is the start DOY of 16-day interval)\n",
    "\n",
    "    # replace the Sub_month values with alternating 1 and 2, which represents first and second half of a month respectively\n",
    "    merged_df['Sub_month'] = [1 if i % 2 == 0 else 2 for i in range(len(merged_df))]\n",
    "\n",
    "    # create a 'Month' column with values from 3 to 11, repeating each value twice\n",
    "    months = [month for month in range(3, 12) for _ in range(2)]  # each month repeats twice\n",
    "    merged_df['Month'] = months[:len(merged_df)]  # ensure the length matches the DataFrame\n",
    "    merged_df['Year'] = year  # add the current year as a new column\n",
    "\n",
    "    # append the merged DataFrame for this year to the final DataFrame\n",
    "    GA90_years = pd.concat([GA90_years, merged_df], ignore_index=True)\n",
    "\n",
    "# select the maximum EVI value for each month to get monthly EVI from 2015 to 2024\n",
    "GA90_max= GA90_years.loc[GA90_years.groupby(['Year', 'Month'])['EVI'].idxmax()].reset_index(drop= True)\n",
    "GA90_max.drop(columns=['Sub_month'], inplace= True) # drop the Sub_month column\n",
    "\n",
    "# save as csv file\n",
    "GA90_max.to_csv('GA_90_Monthly_EVI_2015_to_2024.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
